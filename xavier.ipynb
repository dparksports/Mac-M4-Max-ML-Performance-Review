{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e0e3d5-5a2e-4355-ac2b-9e39a0000398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Matrix:\n",
      "[[0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n",
      "\n",
      "Weight Matrix:\n",
      "[[ 0.83761973  0.26422609 -0.49629848  0.01742475 -0.21748432  1.22340958]\n",
      " [ 0.06931941 -1.30486332 -1.12193157 -0.15673709  0.20024385  0.29320358]\n",
      " [ 1.41757736 -0.35931942 -0.91730274  0.0634401  -2.17883383  0.17773641]\n",
      " [-0.36652114  0.91817341 -1.7816168   0.13421641 -1.45443688  1.13405606]\n",
      " [-0.98803448  0.77298728  0.25119122  1.3846922   0.96035618 -0.79595847]\n",
      " [-1.23000205  0.59510432  0.11899476  0.09155594 -0.52623044  1.02746428]]\n",
      "\n",
      "Logits:\n",
      "[[-0.36652114  0.91817341 -1.7816168   0.13421641 -1.45443688  1.13405606]\n",
      " [ 1.41757736 -0.35931942 -0.91730274  0.0634401  -2.17883383  0.17773641]\n",
      " [-0.36652114  0.91817341 -1.7816168   0.13421641 -1.45443688  1.13405606]\n",
      " [ 0.83761973  0.26422609 -0.49629848  0.01742475 -0.21748432  1.22340958]\n",
      " [ 1.41757736 -0.35931942 -0.91730274  0.0634401  -2.17883383  0.17773641]\n",
      " [ 1.41757736 -0.35931942 -0.91730274  0.0634401  -2.17883383  0.17773641]]\n",
      "\n",
      "Softmax Probabilities:\n",
      "[[0.04310991 0.29795647 0.07851789 0.17590826 0.14494962 0.23549464]\n",
      " [0.25668544 0.08305098 0.18635259 0.1638885  0.07024501 0.09050169]\n",
      " [0.04310991 0.29795647 0.07851789 0.17590826 0.14494962 0.23549464]\n",
      " [0.14372385 0.15493413 0.28390645 0.15651799 0.49936574 0.25750565]\n",
      " [0.25668544 0.08305098 0.18635259 0.1638885  0.07024501 0.09050169]\n",
      " [0.25668544 0.08305098 0.18635259 0.1638885  0.07024501 0.09050169]]\n",
      "\n",
      "Sigmoid Probabilities:\n",
      "[[0.4093819  0.71466978 0.14410361 0.53350382 0.18931966 0.75658665]\n",
      " [0.80495834 0.41112433 0.2855078  0.51585471 0.10166739 0.5443175 ]\n",
      " [0.4093819  0.71466978 0.14410361 0.53350382 0.18931966 0.75658665]\n",
      " [0.69796367 0.56567487 0.37841093 0.50435608 0.44584222 0.77266302]\n",
      " [0.80495834 0.41112433 0.2855078  0.51585471 0.10166739 0.5443175 ]\n",
      " [0.80495834 0.41112433 0.2855078  0.51585471 0.10166739 0.5443175 ]]\n",
      "\n",
      "Softmax Accuracy:\n",
      "0.3333333333333333\n",
      "\n",
      "Sigmoid Accuracy:\n",
      "0.4722222222222222\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 6\n",
    "\n",
    "# Create a one-hot matrix\n",
    "one_hot = np.zeros((N, N))\n",
    "one_hot[np.arange(N), np.random.randint(N, size=N)] = 1\n",
    "\n",
    "# Create a weight matrix with normal distribution\n",
    "mean = 0\n",
    "std_dev = 1\n",
    "weights = np.random.normal(mean, std_dev, (N, N))\n",
    "\n",
    "print(\"One-Hot Matrix:\")\n",
    "print(one_hot)\n",
    "print(\"\\nWeight Matrix:\")\n",
    "print(weights)\n",
    "\n",
    "# Calculate logits\n",
    "logits = one_hot @ weights\n",
    "print(\"\\nLogits:\")\n",
    "print(logits)\n",
    "\n",
    "# Calculate probabilities using softmax\n",
    "softmax_probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=0, keepdims=True)\n",
    "print(\"\\nSoftmax Probabilities:\")\n",
    "print(softmax_probabilities)\n",
    "\n",
    "# Calculate probabilities using sigmoid\n",
    "sigmoid_probabilities = 1 / (1 + np.exp(-logits))\n",
    "print(\"\\nSigmoid Probabilities:\")\n",
    "print(sigmoid_probabilities)\n",
    "\n",
    "# Calculate accuracy for softmax\n",
    "softmax_accuracy = np.mean(np.argmax(softmax_probabilities, axis=0) == np.argmax(one_hot, axis=0))\n",
    "print(\"\\nSoftmax Accuracy:\")\n",
    "print(softmax_accuracy)\n",
    "\n",
    "# Calculate accuracy for sigmoid\n",
    "sigmoid_accuracy = np.mean((sigmoid_probabilities > 0.5) == one_hot)\n",
    "print(\"\\nSigmoid Accuracy:\")\n",
    "print(sigmoid_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b33b9bee-9184-4485-9058-5daaca31469f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Matrix:\n",
      "[[0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n",
      "\n",
      "Weight Matrix:\n",
      "[[ 0.093668    0.52251596  0.41713001  0.37318186 -0.41256222  0.65212732]\n",
      " [ 0.18587193  0.29424073 -0.67724903 -0.17752773  0.01061369 -0.0460084 ]\n",
      " [ 0.39193458  0.63390643  0.28792424  0.13985997 -0.47958734 -0.3864395 ]\n",
      " [ 0.24153936  0.09207504  0.49804628  0.08925517 -0.46829329  0.46301769]\n",
      " [ 0.54849397 -0.01632007  0.67917409 -0.56918654  0.08943282 -0.54221384]\n",
      " [-0.15286447 -0.4434299   0.37622457 -0.42970228  0.14266497  0.38642705]]\n",
      "\n",
      "Logits:\n",
      "tensor([[-0.1529, -0.4434,  0.3762, -0.4297,  0.1427,  0.3864],\n",
      "        [ 0.3919,  0.6339,  0.2879,  0.1399, -0.4796, -0.3864],\n",
      "        [ 0.5485, -0.0163,  0.6792, -0.5692,  0.0894, -0.5422],\n",
      "        [ 0.0937,  0.5225,  0.4171,  0.3732, -0.4126,  0.6521],\n",
      "        [ 0.0937,  0.5225,  0.4171,  0.3732, -0.4126,  0.6521],\n",
      "        [ 0.3919,  0.6339,  0.2879,  0.1399, -0.4796, -0.3864]])\n",
      "\n",
      "PyTorch Softmax Probabilities:\n",
      "tensor([[0.1377, 0.1030, 0.2337, 0.1044, 0.1850, 0.2361],\n",
      "        [0.2071, 0.2637, 0.1866, 0.1609, 0.0866, 0.0951],\n",
      "        [0.2498, 0.1420, 0.2847, 0.0817, 0.1579, 0.0839],\n",
      "        [0.1317, 0.2023, 0.1821, 0.1742, 0.0794, 0.2303],\n",
      "        [0.1317, 0.2023, 0.1821, 0.1742, 0.0794, 0.2303],\n",
      "        [0.2071, 0.2637, 0.1866, 0.1609, 0.0866, 0.0951]])\n",
      "\n",
      "Sigmoid Probabilities:\n",
      "[[0.46185812 0.39092398 0.5929622  0.39419743 0.5356059  0.59542227]\n",
      " [0.5967483  0.65337473 0.5714879  0.5349081  0.38234955 0.40457475]\n",
      " [0.63378614 0.49592006 0.6635543  0.36142454 0.52234334 0.36767274]\n",
      " [0.5233999  0.6277359  0.60279626 0.59222764 0.3982979  0.6574897 ]\n",
      " [0.5233999  0.6277359  0.60279626 0.59222764 0.3982979  0.6574897 ]\n",
      " [0.5967483  0.65337473 0.5714879  0.5349081  0.38234955 0.40457475]]\n",
      "\n",
      "PyTorch Softmax Accuracy:\n",
      "tensor(0.1667)\n",
      "\n",
      "Sigmoid Accuracy:\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "N = 6\n",
    "\n",
    "# Create a one-hot matrix\n",
    "one_hot = np.zeros((N, N))\n",
    "one_hot[np.arange(N), np.random.randint(N, size=N)] = 1\n",
    "\n",
    "# Create a weight matrix using Xavier initialization\n",
    "def xavier_init(shape):\n",
    "    epsilon = np.sqrt(6) / np.sqrt(np.sum(shape))\n",
    "    return np.random.uniform(-epsilon, epsilon, shape)\n",
    "\n",
    "weights = xavier_init((N, N))\n",
    "\n",
    "print(\"One-Hot Matrix:\")\n",
    "print(one_hot)\n",
    "print(\"\\nWeight Matrix:\")\n",
    "print(weights)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "one_hot_tensor = torch.tensor(one_hot, dtype=torch.float32)\n",
    "weights_tensor = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "# Calculate logits\n",
    "logits = torch.matmul(one_hot_tensor, weights_tensor)\n",
    "print(\"\\nLogits:\")\n",
    "print(logits)\n",
    "\n",
    "# Calculate probabilities using PyTorch softmax\n",
    "torch_softmax_probabilities = F.softmax(logits, dim=1)\n",
    "print(\"\\nPyTorch Softmax Probabilities:\")\n",
    "print(torch_softmax_probabilities)\n",
    "\n",
    "# Calculate probabilities using sigmoid\n",
    "sigmoid_probabilities = 1 / (1 + np.exp(-logits.numpy()))\n",
    "print(\"\\nSigmoid Probabilities:\")\n",
    "print(sigmoid_probabilities)\n",
    "\n",
    "# Calculate accuracy for PyTorch softmax\n",
    "torch_softmax_accuracy = torch.mean((torch.argmax(torch_softmax_probabilities, dim=1) == torch.argmax(one_hot_tensor, dim=1)).float())\n",
    "print(\"\\nPyTorch Softmax Accuracy:\")\n",
    "print(torch_softmax_accuracy)\n",
    "\n",
    "# Calculate accuracy for sigmoid\n",
    "sigmoid_accuracy = np.mean((sigmoid_probabilities > 0.5) == one_hot)\n",
    "print(\"\\nSigmoid Accuracy:\")\n",
    "print(sigmoid_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664257e8-b197-497f-b377-2c47130534eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Probabilities (NumPy):\n",
      "[[0.19042168 0.08280869 0.09543444 0.13446466 0.25209838 0.24477215]\n",
      " [0.19042168 0.08280869 0.09543444 0.13446466 0.25209838 0.24477215]\n",
      " [0.08122944 0.17414603 0.0832448  0.3233764  0.11396337 0.22403996]\n",
      " [0.19042168 0.08280869 0.09543444 0.13446466 0.25209838 0.24477215]\n",
      " [0.08122944 0.17414603 0.0832448  0.3233764  0.11396337 0.22403996]\n",
      " [0.08122944 0.17414603 0.0832448  0.3233764  0.11396337 0.22403996]]\n",
      "\n",
      "Softmax Probabilities (PyTorch):\n",
      "tensor([[0.1904, 0.0828, 0.0954, 0.1345, 0.2521, 0.2448],\n",
      "        [0.1904, 0.0828, 0.0954, 0.1345, 0.2521, 0.2448],\n",
      "        [0.0812, 0.1741, 0.0832, 0.3234, 0.1140, 0.2240],\n",
      "        [0.1904, 0.0828, 0.0954, 0.1345, 0.2521, 0.2448],\n",
      "        [0.0812, 0.1741, 0.0832, 0.3234, 0.1140, 0.2240],\n",
      "        [0.0812, 0.1741, 0.0832, 0.3234, 0.1140, 0.2240]], dtype=torch.float64)\n",
      "\n",
      "Sigmoid Probabilities:\n",
      "[[0.57509597 0.37050888 0.40416806 0.48868544 0.64181535 0.63500759]\n",
      " [0.57509597 0.37050888 0.40416806 0.48868544 0.64181535 0.63500759]\n",
      " [0.33584421 0.52017582 0.34133259 0.66811458 0.41501538 0.58241067]\n",
      " [0.57509597 0.37050888 0.40416806 0.48868544 0.64181535 0.63500759]\n",
      " [0.33584421 0.52017582 0.34133259 0.66811458 0.41501538 0.58241067]\n",
      " [0.33584421 0.52017582 0.34133259 0.66811458 0.41501538 0.58241067]]\n",
      "\n",
      "Accuracy Comparison:\n",
      "NumPy Softmax Accuracy: 0.0000\n",
      "PyTorch Softmax Accuracy: 0.0000\n",
      "Sigmoid Accuracy: 0.5000\n",
      "Softmax accuracies match.\n",
      "Softmax and Sigmoid accuracies differ.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "N = 6\n",
    "\n",
    "# Create a one-hot matrix\n",
    "one_hot = np.zeros((N, N))\n",
    "one_hot[np.arange(N), np.random.randint(N, size=N)] = 1\n",
    "\n",
    "# Create a weight matrix using Xavier initialization\n",
    "def xavier_init(shape):\n",
    "    epsilon = np.sqrt(6) / np.sqrt(np.sum(shape))\n",
    "    return np.random.uniform(-epsilon, epsilon, shape)\n",
    "\n",
    "weights = xavier_init((N, N))\n",
    "\n",
    "# Calculate logits\n",
    "logits = one_hot @ weights\n",
    "\n",
    "## Method 1: NumPy Softmax\n",
    "softmax_probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "numpy_softmax_accuracy = np.mean(np.argmax(softmax_probabilities, axis=1) == np.argmax(one_hot, axis=1))\n",
    "\n",
    "## Method 2: PyTorch Softmax\n",
    "torch_logits = torch.tensor(logits)\n",
    "torch_softmax_probabilities = torch.nn.functional.softmax(torch_logits, dim=1)\n",
    "torch_softmax_accuracy = torch.mean((torch.argmax(torch_softmax_probabilities, dim=1) == torch.argmax(torch.tensor(one_hot), dim=1)).float())\n",
    "\n",
    "## Method 3: Sigmoid\n",
    "sigmoid_probabilities = 1 / (1 + np.exp(-logits))\n",
    "sigmoid_accuracy = np.mean((sigmoid_probabilities > 0.5) == one_hot)\n",
    "\n",
    "print(\"Softmax Probabilities (NumPy):\")\n",
    "print(softmax_probabilities)\n",
    "print(\"\\nSoftmax Probabilities (PyTorch):\")\n",
    "print(torch_softmax_probabilities)\n",
    "print(\"\\nSigmoid Probabilities:\")\n",
    "print(sigmoid_probabilities)\n",
    "\n",
    "print(\"\\nAccuracy Comparison:\")\n",
    "print(f\"NumPy Softmax Accuracy: {numpy_softmax_accuracy:.4f}\")\n",
    "print(f\"PyTorch Softmax Accuracy: {torch_softmax_accuracy.item():.4f}\")\n",
    "print(f\"Sigmoid Accuracy: {sigmoid_accuracy:.4f}\")\n",
    "\n",
    "if np.isclose(numpy_softmax_accuracy, torch_softmax_accuracy.item(), atol=1e-4):\n",
    "    print(\"Softmax accuracies match.\")\n",
    "else:\n",
    "    print(\"Softmax accuracies differ.\")\n",
    "\n",
    "if np.isclose(numpy_softmax_accuracy, sigmoid_accuracy, atol=1e-4):\n",
    "    print(\"Softmax and Sigmoid accuracies match.\")\n",
    "else:\n",
    "    print(\"Softmax and Sigmoid accuracies differ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa47b3-efc9-44a2-b836-5d5daa5952e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
